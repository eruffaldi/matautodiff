

H(x' A x) = A+A'
H(tr AX'BX) = 1/2 (A' kron B + A kron B')
H(tr X^2)  =  ...
H(a' X X' a) = ...
H(inv X) = Kn (X'^-2 kron inv X + inv X' kron X^{-2})
H(|X|) = ...
H(f' A f) = ...


---------
Note: The calculation of gradients by nonincremental reverse makes the corresponding computational graph symmetric, a property that should be exploited and maintained in accumulating Hessians.
-------
Problem: at the end we have: [k,l,m,n,m,n] where m,n is for all variables, and the whole point that when we accumulate we do accumulate only toward a given one and this means that we should start with [k,l,m1,n1] function (previous adjoint) and getting to ward th end but this is not working
-------



http://www.tc.umn.edu/~nydic001/docs/unpubs/Magnus_Matrix_Differentials_Presentation.pdf

http://select.cs.cmu.edu/class/10725-S10/recitations/r4/Matrix_Calculus_Algebra.pdf

http://www.math.harvard.edu/archive/21a_summer_05/handouts/lecture9.pdf

https://books.google.it/books?id=N871f_bp810C&pg=PA353&lpg=PA353&dq=hessian+chain+rule&source=bl&ots=st28mYrWrY&sig=ZNuWzqFIIVmf6Rc6INBh3dmRIU8&hl=en&sa=X&ved=0ahUKEwiA25nOv-HMAhXBJ5oKHdsxA1AQ6AEIITAC#v=onepage&q=hessian%20chain%20rule&f=false

Gower (computation graph): http://www.ime.unicamp.br/rel_pesq/2010/pdf/rp16-10.pdf

Griewank and Waltherâ€™s reverse Hessian computation algorithm: 

------
e.g.
!! X + Y 
	! X + Y = dX + dY 
	!! X + Y = 0
!! (Ax)'C(Dx) = A'CD + D'C'A
	nb !(Ax)' = Tmn !(Ax) = Tmn kron(A,I) !x
	! (Ax)'C(Dx) = kron(I,C Dx) !(Ax)' + kron(I,(Ax)') !(CDx) = kron(I, (C Dx)') !()
	!! (Ax)'C(Dx) = ... !(CDx) !(Ax') + ... !(Ax)' !(CDx) = ... C D !x Tmn kron(A,I) !x + ... Tmn kron(A,I) !x kron(CD,I) !x
	we need a rule
		A B C D => A ... C B D  in particular   C D ... Tmn kron(A,I) !x !x
!! x'x = 2I
	! x' x = kron(x',I) d(x') + kron(I,x') d x = kron(x',I) Tmn dx + kron(I,x') dx  
	!! ... d(x') Tmn dx + ... d(x') dx
!! (Y')
	! Y' = Tmn dY
	!! Y' = 0

NOTE: we are not following classic matrix operations but we work with the vectorized operations
	express matrix multiplication as a linear transformation on matrices

	with A kl,B lm,C mn
	C = A + B     =>  C: = A: + B:  / idem for the elementwise
	C = A * B     =>  C: = kron (B',Ik) A: = kron(Im,A) B:
                      alt: C = (B' A')' => C: = Tmn kron(A,Il) B': = Tmn kron(A,Il) Tkl B:
	C = A * B * C =>  C: = kron (C',A) B: = kron (In, AB) C: = kron(C'B',Ik) A:
	C = A'        =>  C: = Tmn A:
	C = diag(A)   =>  C: = Snn A:     # Snn select the elements that are in the diagonal
	C = trace(A)  =>  C: = ones(1,n) Snn A:
	...

	using the above we obtain some formulation that highlight the term on the right:

	d(AB): = kron(Im,A) dB: + kron(B',Ik) dA:
	d(kron(A,B)): = kron(A,dB)+kron(dA,B) = Q(A) dB: + Q(B) dA: with Q special matrix
	dd(AB): = d(kron(Im,A)) dB: + d(kron(B',Ik)) dA: = Q(Im) dA: dB: + Q(Ik) Tmn dB: dA:
		se A=(YX)' B=ZX
		dA = Tmn kron(Y,I) dX:
		dB = kron(Z,I) dX:
		Q(Im) Tmn kron(Y,I) dX: kron(Z,I) dX: + Q(Ik) Tmn kron(Z,I) dX: kron(Y,I) dX: ==> (Y'Z+Z'Y):

----------------

e.g. (x1x0)(x1+x0)
output is scalar J is [1,n]
n lowest level variable => H is [n,n]
l intermediate steps 
f=v1v2
v1=x1x0
v2=x1+x0

w = state vector [n+l]
f = selectlast Phi_l * Phi_l-1 .... Phi1 (P' x)  [3]
	where P puts x in the state vector (first n elements)

Chain rule can be applied to [3] as:

Df' = selectlast' DPhi_l DPhi_l-1 ... DPhi_1 (P'x)P'

--------------------------------------------------------------------

Existing in Julia: http://www.juliadiff.org/

- forward autodiff using Dual numbers or HyperDual numbers for 1st and 2nd, improved in ForwardDiff
- taylor based is TaylorSeries

- ReverseDiff 
- ReverseDiffSparse

The most related is: 
https://github.com/JuliaDiff/ReverseDiffSource.jl
http://reversediffsourcejl.readthedocs.io/en/master/deriv_rule.html

Defining new functions : @deriv_rule()

DETAIL https://github.com/JuliaDiff/ReverseDiffSource.jl/blob/master/src/rdiff.jl

- high order => multiple variables
- first perform the first order
- then reprepare the graph restructuring everything as:
	byorder	
		by dg.nodes
			by n.parents

- what does reversegraph?
- NOTE: are they working on ...

-----------

f (u(w(x)))
	k,l for u
	i,j for w
J2(f,x,y) = sum_k J(f,uk) J2(uk,x,y) + sum_kl J2(f,uk,ul) J(uk,x,y) J(ul,x,y)
J2(f,x,y) = 
	sum_k J(f,uk) (	
		sum_j J(uk,wj) J2(wj,x,y) 
		+ 
		sum_ij J2(uk,wi,wj) 
			BLOCK1: prefixed with f2_pre
				sum_i J(uk,wi) J(wi,x)
				*
				sum_j J(uk,wj) J(wj,x)
		) 
	+ 
	sum_kl J2(f,uk,ul) 
		(
		sum_i J(uk,wi) J(wi,x)  
		*
		sum_j J(ul,wj) J(wj,y)
		)

We propagate J2(wj,*) easy = f2_pre J(f,uk) J(uk,wj) 

We propagate down with J(wi,*)*J(wj,*) taken several times
	f2_pre  J(f,uk) J2(uk,wi,wj) J(uk,wi) 

Approach of Gawer:
	The interdependence relations are thus translated into predecessor
	relations between nodes, and are denoted by the symbol ~<

	foreach i topdown
		#push
		foreach p<=i with w(pi) != 0 === traverse all the descendents
			if p!=i
				foreach j operands ~< of i
					w(jp) += (j==p?2:1) J(phi_i,v_j) w(pi)
			else
				foreach unordered jk operands ~< of i
					w(jk) += J(phi_i,v_j) J(phi_i,v_k) w(ii)

		#create: this pushes down, and corresponds to the second part of our expression [[ sum_jk J2(f,uj,uk) J(uj,x) J(uk,y) ]] saying that we propagate down the connection between jk for all the subsequent derivatives
		foreach unordered j,k operands ~< of i
			w(jk) += adjoint_i J2(phi_i,vk,vj)

		#second order adjoint corresponding to the down propagation of: sum_k J(f,uk) J2(uk,x,y)
		foreach j operand ~< of i
			adjoint_j += adjoint_i J(phi_i,vj)

	output f'' = PWP' 
		P [n, n+l] with all zero except eye(n) on the left (aka selector)

Matrix Work!
	each phi_i is: [lir,lic] giving lir*lic=Li with inputs [lkr,lkc] total input sum lkr*lkc = sum Lk
	final f has index l so Ll
	adjoint_i is: [Ll,Li]
	use another support matrix for marking w(pi) != 0 efficient

	w(jk) block in create is: [Ll,Li] * [Li, Lk+Lj] = [Ll,Lk+Lj]   OK and EASY

	w(jk) += J(phi_i,v_j) J(phi_i,v_k) w(ii)
		[Ll,Lk+Lj] += [Ll, Li+Li] * [Li,Lj] *?* [Li,Lk]
			if it was tensor: [Ll,Li,Li] ? [Li,Lj] ? [Li,Lk] == [Ll,Li,Lj] ? [Li,Lk] == [Ll,Lk+Lj] MAYBE

	w(jp) += ... J(phi_i,v_j) w(pi)
		[Ll,Lj+Lp] += [Ll, Lp+Li] *?* [Li,Lj]
			if it was tensor: [Ll,Lp,Li] * [Li,Lj] = [Ll,Lp,Lj] OK

	Total size of W: we have a block per expressiona (n+l) but each block wjp is [Ll,Lj+Lp], but W should be squared!?

Note: operands can be any (e.g. matrix build)

If we flatten we need to allocate all operands